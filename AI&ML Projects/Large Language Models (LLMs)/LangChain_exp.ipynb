{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summaries Of Short **Text**"
      ],
      "metadata": {
        "id": "-VAfs_QnXIVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "FmWyQQUzHukf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "p4K3FV8gT--1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "KyLSQ-osUOQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade langchain"
      ],
      "metadata": {
        "id": "tfTUTnwkVeUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key='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'"
      ],
      "metadata": {
        "id": "lhfSCKdPVsgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Note, the default model is already 'text-davinci-003' but I call it out here explicitly so you know where to change it later if you want\n",
        "llm = OpenAI(temperature=0, model_name = 'text-davinci-003', openai_api_key=openai_api_key)\n",
        "\n",
        "# Create our template\n",
        "template = \"\"\"\n",
        "%INSTRUCTIONS:\n",
        "Please summarize the following piece of text.\n",
        "Respond in a manner that a 5 year old would understand.\n",
        "\n",
        "%TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# Create a LangChain prompt template that we can insert values to later\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "YZ4lk17wVyT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusing_text = \"\"\"\n",
        "For the next 130 years, debate raged.\n",
        "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
        "‚ÄúThe problem is that when you look up close at the anatomy, it‚Äôs evocative of a lot of different things, but it‚Äôs diagnostic of nothing,‚Äù says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
        "‚ÄúAnd it‚Äôs so damn big that when whenever someone says it‚Äôs something, everyone else‚Äôs hackles get up: ‚ÄòHow could you have a lichen 20 feet tall?‚Äô‚Äù\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lWt8sQOcV2lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"------- Prompt Begin -------\")\n",
        "\n",
        "final_prompt = prompt.format(text=confusing_text)\n",
        "print(final_prompt)\n",
        "\n",
        "print (\"------- Prompt End -------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXSA6xnNV7Kf",
        "outputId": "acf62535-aa34-47d8-c62c-2c69524a7994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Prompt Begin -------\n",
            "\n",
            "%INSTRUCTIONS:\n",
            "Please summarize the following piece of text.\n",
            "Respond in a manner that a 5 year old would understand.\n",
            "\n",
            "%TEXT:\n",
            "\n",
            "For the next 130 years, debate raged.\n",
            "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
            "‚ÄúThe problem is that when you look up close at the anatomy, it‚Äôs evocative of a lot of different things, but it‚Äôs diagnostic of nothing,‚Äù says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
            "‚ÄúAnd it‚Äôs so damn big that when whenever someone says it‚Äôs something, everyone else‚Äôs hackles get up: ‚ÄòHow could you have a lichen 20 feet tall?‚Äô‚Äù\n",
            "\n",
            "\n",
            "------- Prompt End -------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(final_prompt)\n",
        "print (output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdQ-_2QWPY0",
        "outputId": "24135d88-d04b-4b02-f71e-9f05970cfb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For 130 years, people argued about what Prototaxites was. Some thought it was a lichen, some thought it was a fungus, and some thought it was a tree. But no one could agree because when you looked closely, it looked like a lot of different things. It was also very big, so people were surprised that it could be a lichen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summaries Of Longer Text**"
      ],
      "metadata": {
        "id": "lDcF6TUnXUem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "no4ULFZ4X3GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question and Answers**"
      ],
      "metadata": {
        "id": "NhzdvSymZrv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "3DoXWOYEZu4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Rachel is 30 years old\n",
        "Bob is 45 years old\n",
        "Kevin is 65 years old\n",
        "\"\"\"\n",
        "\n",
        "question = \"Who is under 40 years old?\""
      ],
      "metadata": {
        "id": "Yfs8kEuavYWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(context + question)\n",
        "\n",
        "# I strip the text to remove the leading and trailing whitespace\n",
        "print (output.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0nFTmhyvamP",
        "outputId": "0a4cf72d-3c0f-43cc-8720-beaa8dbbb66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel is under 40 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "# The vectorstore we'll be using\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# The LangChain component we'll use to get the documents\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# The easy document loader for text\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# The embedding engine that will convert our text to vectors\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "BO9d5BVVw3C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4D9d2QvIfxD",
        "outputId": "9ff32026-7142-4d4a-883b-bd86377ec9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('/content/drive/My Drive/Colab Notebooks/Data/sample.txt')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
      ],
      "metadata": {
        "id": "HDfP1rwNxB_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f78e89-a49a-4678-a044-aba03045319a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 document\n",
            "You have 17229 characters in that document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
        "docs = text_splitter.split_documents(doc)"
      ],
      "metadata": {
        "id": "0YugJUCyxVv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of characters so we can see the average later\n",
        "num_total_characters = sum([len(x.page_content) for x in docs])\n",
        "\n",
        "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7suidZ5xWVV",
        "outputId": "7c1ff5f0-ea92-4801-b768-bdf7619f3b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 7 documents that have an average of 2,774 characters (smaller pieces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "id": "KJo9cas6ycj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE4vd_xNylCf",
        "outputId": "f3498cae-7cb9-4a11-c391-504aa4b2267a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your embeddings engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
        "docsearch = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "7z8JTSbexnRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "PDIcr6vcxpVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What happened to the author in the final day of his sophomore year of high school?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z6ZgcIDGxrN4",
        "outputId": "9de0c509-5434-4d99-90f9-9e3c528204f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The author was hit in the face with a baseball bat, which caused him to have a broken nose, multiple skull fractures, and two shattered eye sockets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vanilla Extraction**"
      ],
      "metadata": {
        "id": "xWOh5CYuzGO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To help construct our Chat Messages\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To parse outputs and get structured data back\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "9Dc4UkzKzkDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
        "Return the fruit name and emojis in a python dictionary\n",
        "\"\"\"\n",
        "\n",
        "fruit_names = \"\"\"\n",
        "Apple, Pear, this is an kiwi\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vgkxiE_KzJMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your prompt which combines the instructions w/ the fruit names\n",
        "prompt = (instructions + fruit_names)\n",
        "\n",
        "# Call the LLM\n",
        "output = chat_model([HumanMessage(content=prompt)])\n",
        "\n",
        "print (output.content)\n",
        "print (type(output.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53qYC-rYzMMz",
        "outputId": "f0c60414-ff62-416b-cfae-b30e77fb4006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Apple': 'üçé', 'Pear': 'üçê', 'kiwi': 'ü•ù'}\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = eval(output.content)\n",
        "\n",
        "print (output_dict)\n",
        "print (type(output_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivbq-6sbzOJM",
        "outputId": "8ab9ffef-f397-4971-ea19-d17ab798d068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Apple': 'üçé', 'Pear': 'üçê', 'kiwi': 'ü•ù'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Response Schema**"
      ],
      "metadata": {
        "id": "V0vWdnicAN9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The schema I want out\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
        "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
        "]\n",
        "\n",
        "# The parser that will look for the LLM output in my schema and return it back to me\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "Z7yjzHFRAQYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The format instructions that LangChain makes. Let's look at them\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaBQTx-iASSx",
        "outputId": "aa98aed5-2f9a-4909-85b1-49853e9dbead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt template that brings it all together\n",
        "# Note: This is a different prompt template than before because we are using a Chat Model\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
        "                                                    {format_instructions}\\n{user_prompt}\")\n",
        "    ],\n",
        "    input_variables=[\"user_prompt\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "4Ao7m68LAUAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_query = prompt.format_prompt(user_prompt=\"I really like Golden hour by JVKE\")\n",
        "print (fruit_query.messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HcbmbpzAVvK",
        "outputId": "e95b1776-0d8c-4a9f-c63f-c8422f5987d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a command from the user, extract the artist and song names \n",
            "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n",
            "I really like Golden hour by JVKE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_output = chat_model(fruit_query.to_messages())\n",
        "output = output_parser.parse(fruit_output.content)\n",
        "\n",
        "print (output)\n",
        "print (type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iri-aZPAXjC",
        "outputId": "183be111-d4f5-41ea-aa03-15202f041a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artist': 'JVKE', 'song': 'Golden hour'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    }
  ]
}